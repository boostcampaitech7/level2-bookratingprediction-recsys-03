{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "import optuna\n",
    "import regex\n",
    "import json\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WanDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = str(input('run 이름을 입력하세요 :'))\n",
    "# selected_model = str(input('model 명을 입력하세요 (xgb/rf) :'))\n",
    "opt = bool(input('Optuna 사용 여부를 입력하세요 (뭐라도 입력 시 사용) :'))\n",
    "\n",
    "wandb.init(\n",
    "    settings=wandb.Settings(start_method='thread'),\n",
    "    dir=None,  # 로컬에 로그 저장하지 않음\n",
    "    entity='remember-us', # team name,\n",
    "    project='active', # project name\n",
    "    name=run, # run name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path: str = '/data/ephemeral/home/book/code/data/'\n",
    "users = pd.read_csv(data_path + 'users.csv')\n",
    "books = pd.read_csv(data_path + 'books.csv')\n",
    "train = pd.read_csv(data_path + 'train_ratings.csv')\n",
    "test = pd.read_csv(data_path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(data_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(x: str) -> list:\n",
    "    '''문자열을 리스트로 변환하는 함수'''\n",
    "    return x[2:-2].split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(x: str) -> list:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : str\n",
    "        location 데이터\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : list\n",
    "        location 데이터를 나눈 뒤, 정제한 결과를 반환합니다.\n",
    "        순서는 country, state, city, ... 입니다.\n",
    "    '''\n",
    "    res = x.split(',')\n",
    "    res = [i.strip().lower() for i in res]\n",
    "    res = [regex.sub(r'[^a-zA-Z/ ]', '', i) for i in res]  # remove special characters\n",
    "    res = [i if i not in ['n/a', ''] else np.nan for i in res]  # change 'n/a' into NaN\n",
    "    res.reverse()  # reverse the list to get country, state, city, ... order\n",
    "\n",
    "    for i in range(len(res)-1, 0, -1):\n",
    "        if (res[i] in res[:i]) and (not pd.isna(res[i])):  # remove duplicated values if not NaN\n",
    "            res.pop(i)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(summary_text: str) -> str:\n",
    "    '''\n",
    "    주어진 텍스트 요약을 전처리합니다.\n",
    "\n",
    "    1. 특수 문자 제거\n",
    "    2. 알파벳과 숫자, 공백을 제외한 모든 문자 제거\n",
    "    3. 여러 개의 공백을 하나의 공백으로\n",
    "    4. 문자열의 앞뒤 공백 제거\n",
    "    5. 모든 문자를 소문자로 변환\n",
    "\n",
    "    Args:\n",
    "        summary_text (str): 전처리할 텍스트 문자열\n",
    "\n",
    "    Returns:\n",
    "        str: 전처리된 텍스트 문자열. 입력이 NaN인 경우 'unknown' 반환.\n",
    "    '''\n",
    "    if pd.isna(summary_text):\n",
    "        return 'unknown'  # NaN일 경우 'unknown' 반환\n",
    "    \n",
    "    summary_text = regex.sub('[.,\\'\\'''\\'!?]', '', summary_text)  # 특수 문자 제거\n",
    "    summary_text = regex.sub('[^0-9a-zA-Z\\s]', '', summary_text)  # 알파벳과 숫자, 공백 제외한 문자 제거\n",
    "    summary_text = regex.sub('\\s+', ' ', summary_text)  # 여러 개의 공백을 하나의 공백으로\n",
    "    summary_text = summary_text.lower()  # 소문자로 변환\n",
    "    summary_text = summary_text.strip()  # 앞뒤 공백 제거\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_publication(x: int, a: int) -> int:\n",
    "    '''\n",
    "    주어진 연도를 특정 기준에 따라 카테고리화하는 함수입니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : int\n",
    "        책의 발행 연도.\n",
    "    a : int\n",
    "        연도를 그룹화할 때 사용할 기준값 (예: 5년 단위로 그룹화).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        카테고리화된 연도를 반환합니다. \n",
    "        - 1970년 이하의 연도는 1970으로 반환합니다.\n",
    "        - 2000년 초과의 연도는 2006으로 반환합니다.\n",
    "        - 나머지 연도는 a 값에 맞게 그룹화하여 반환합니다.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    books['years'] = books['year_of_publication'].apply(lambda x: categorize_publication(x, 5))\n",
    "    print(books['years'].value_counts())\n",
    "    '''\n",
    "    if x <= 1970:\n",
    "        return 1970\n",
    "    elif x > 2000:\n",
    "        return 2006\n",
    "    else:\n",
    "        return x // a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_language_from_isbn(isbn):\n",
    "    '''\n",
    "    ISBN 정보를 사용하여 언어 코드를 추출하는 함수입니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    isbn : str\n",
    "        책의 ISBN 번호.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        ISBN에서 추출한 언어 코드. ISBN이 비어있거나 형식에 맞지 않을 경우 최빈값 'en'을 반환합니다.\n",
    "        - isbn_language_map 참고\n",
    "        - 기타 언어 코드: isbn_language_map에 정의된 국가 코드를 기반으로 반환\n",
    "    '''\n",
    "    isbn_language_map = {\n",
    "        '0': 'en', '1': 'en', '2': 'fr', '3': 'de', '4': 'ja',\n",
    "        '5': 'ru', '7': 'zh-CN', '82': 'no', '84': 'es', '87': 'da',\n",
    "        '88': 'it', '89': 'ko', '94': 'nl', '600': 'fa', '602': 'ms',\n",
    "        '606': 'ro', '604': 'vi', '618': 'el', '967': 'ms', '974': 'th',\n",
    "        '989': 'pt'\n",
    "    }\n",
    "    if not isbn or not isbn.isdigit():\n",
    "        return 'en'  # 기본값 영어권\n",
    "    for prefix, language in isbn_language_map.items():\n",
    "        if isbn.startswith(prefix):\n",
    "            return language\n",
    "    return 'en'  # 기본값 영어권"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_language_using_isbn(books):\n",
    "    '''\n",
    "    ISBN 정보를 활용하여 language 결측치를 대체하는 함수입니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    books : pd.DataFrame\n",
    "        책 정보가 담긴 DataFrame. 반드시 'isbn' 및 'language' 열을 포함해야 합니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        language 결측치가 ISBN 정보를 사용해 대체된 DataFrame. ISBN에서 언어를 추출할 수 없는 경우\n",
    "        기본값 'en'으로 대체됩니다.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    books = replace_language_using_isbn(books)\n",
    "    '''\n",
    "    books['extracted_language'] = books['isbn'].apply(extract_language_from_isbn)\n",
    "    books['language'] = books.apply(\n",
    "        lambda row: row['extracted_language'] if pd.isna(row['language']) else row['language'],\n",
    "        axis=1\n",
    "    )\n",
    "    books.drop(columns=['extracted_language'], inplace=True)\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(x: int, a: int) -> int:\n",
    "    '''\n",
    "    주어진 나이를 특정 기준에 따라 카테고리화하는 함수입니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : int\n",
    "        유저의 나이.\n",
    "    a : int\n",
    "        나이를 그룹화할 때 사용할 기준값 (예: 10년 단위로 그룹화).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        카테고리화된 나이를 반환합니다. \n",
    "        - 20년 미만의 나이는 10으로 반환합니다.\n",
    "        - 60년 이상의 나이는 60으로 반환합니다.\n",
    "        - 나머지 나이는 a 값에 맞게 그룹화하여 반환합니다.\n",
    "    '''\n",
    "    if x < 20:\n",
    "        return 10\n",
    "    elif x >= 60:\n",
    "        return 60\n",
    "    else:\n",
    "        return x // a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ = users.copy()\n",
    "books_ = books.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books 데이터 전처리\n",
    "# book_title, book_author, publisher 열의 텍스트를 전처리\n",
    "books_['book_title'] = books_['book_title'].apply(text_preprocessing)\n",
    "books_['book_author'] = books_['book_author'].apply(text_preprocessing)\n",
    "books_['publisher'] = books_['publisher'].apply(text_preprocessing)\n",
    "\n",
    "# 발행 연도를 특정 기준으로 카테고리화하여 publication_range 열에 저\n",
    "books_['publication_range'] = books_['year_of_publication'].apply(lambda x: categorize_publication(x, 5))\n",
    "\n",
    "# ISBN 정보를 사용하여 결측된 language 열을 대체\n",
    "books_ = replace_language_using_isbn(books_)\n",
    "\n",
    "# category 열의 첫 번째 항목만 사용하며, 결측치가 있으면 NaN으로 설정\n",
    "books_['category'] = books_['category'].apply(lambda x: str2list(x)[0] if not pd.isna(x) else np.nan)\n",
    "\n",
    "# category 열의 텍스트를 전처리\n",
    "books_['category'] = books_['category'].apply(text_preprocessing)\n",
    "\n",
    "# 상위 카테고리 목록을 정의\n",
    "high_categories = ['fiction', 'biography', 'history', 'religion', 'nonfiction', 'social', 'science', 'humor', 'body', \n",
    "                'business', 'economics', 'cook', 'health', 'fitness', 'famil', 'relationship', \n",
    "                'computer', 'travel', 'selfhelp', 'psychology', 'poetry', 'art', 'critic', 'nature', 'philosophy', \n",
    "                'reference','drama', 'sports', 'politic', 'comic', 'novel', 'craft', 'language', 'education', 'crime', 'music', 'pet', \n",
    "                'child', 'collection', 'mystery', 'garden', 'medical', 'author', 'house','technology', 'engineering', 'animal', 'photography',\n",
    "                'adventure', 'game', 'science fiction', 'architecture', 'law', 'fantasy', 'antique', 'friend', 'brother', 'sister', 'cat',\n",
    "                'math', 'christ', 'bible', 'fairy', 'horror', 'design', 'adolescence', 'actor', 'dog', 'transportation', 'murder', 'adultery', 'short', 'bear'\n",
    "                ]\n",
    "\n",
    "# high_category 열을 초기화\n",
    "books_['high_category'] = None\n",
    "\n",
    "# 각 카테고리에 대해 반복하며 매핑\n",
    "for high_category in high_categories:\n",
    "    # category 열에서 high_category가 포함된 행을 찾고, 해당 행의 high_category 열을 업데이트\n",
    "    books_.loc[books_['category'].str.contains(high_category, case=False, na=False), 'high_category'] = high_category\n",
    "books_['high_category'] = books_['high_category'].fillna('others') # 결측치를 'others'로 대체\n",
    "\n",
    "# users 데이터 전처리\n",
    "# age 열의 결측치를 평균값으로 대체\n",
    "users_['age'] = users_['age'].fillna(users_['age'].mean())\n",
    "\n",
    "# 나이를 특정 기준으로 카테고리화하여 age_range 열에 저장\n",
    "users_['age_range'] = users_['age'].apply(lambda x: categorize_age(x, 10))\n",
    "\n",
    "# location 데이터를 리스트로 분리하여 location_list 열에 저장\n",
    "users_['location_list'] = users_['location'].apply(lambda x: split_location(x)) \n",
    "\n",
    "# location_list에서 첫 번째 요소를 location_country 열로, 두 번째 요소를 location_state 열로, 세 번째 요소를 location_city 열로 설정\n",
    "users_['location_country'] = users_['location_list'].apply(lambda x: x[0])\n",
    "users_['location_state'] = users_['location_list'].apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "users_['location_city'] = users_['location_list'].apply(lambda x: x[2] if len(x) > 2 else np.nan)\n",
    "\n",
    "# 각 행을 반복하며 결측된 location_country나 location_state 값을 보완\n",
    "for idx, row in users_.iterrows():\n",
    "    if (not pd.isna(row['location_state'])) and pd.isna(row['location_country']):\n",
    "        fill_country = users_[users_['location_state'] == row['location_state']]['location_country'].mode()\n",
    "        fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "        users_.loc[idx, 'location_country'] = fill_country\n",
    "    elif (not pd.isna(row['location_city'])) and pd.isna(row['location_state']):\n",
    "        if not pd.isna(row['location_country']):\n",
    "            fill_state = users_[(users_['location_country'] == row['location_country']) \n",
    "                                & (users_['location_city'] == row['location_city'])]['location_state'].mode()\n",
    "            fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "            users_.loc[idx, 'location_state'] = fill_state\n",
    "        else:\n",
    "            fill_state = users_[users_['location_city'] == row['location_city']]['location_state'].mode()\n",
    "            fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "            fill_country = users_[users_['location_city'] == row['location_city']]['location_country'].mode()\n",
    "            fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "            users_.loc[idx, 'location_country'] = fill_country\n",
    "            users_.loc[idx, 'location_state'] = fill_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_country 열의 최빈값을 계산\n",
    "most_frequent_country = users_['location_country'].mode()[0]\n",
    "# NaN 값을 최빈값으로 대체\n",
    "users_['location_country'] = users_['location_country'].fillna(most_frequent_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user,book에서 사용할 열 정의\n",
    "users_final = users_[['user_id', 'age', 'age_range', 'location_country']]\n",
    "books_final = books_[['isbn', 'book_title', 'book_author', 'publisher', 'language', 'high_category', 'publication_range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합\n",
    "train = train.merge(users_final, on='user_id').merge(books_final, on='isbn')\n",
    "test = test.merge(users_final, on='user_id').merge(books_final, on='isbn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id 별 review_counts 계산\n",
    "user_id_counts = train['user_id'].value_counts()\n",
    "train['user_review_counts'] = train['user_id'].map(user_id_counts)\n",
    "test['user_review_counts'] = test['user_id'].map(user_id_counts)\n",
    "test['user_review_counts'] = test['user_review_counts'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isbn 별 review_counts 계산\n",
    "book_isbn_counts = train['isbn'].value_counts()\n",
    "train['book_review_counts'] = train['isbn'].map(book_isbn_counts)\n",
    "test['book_review_counts'] = test['isbn'].map(book_isbn_counts)\n",
    "test['book_review_counts'] = test['book_review_counts'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파생변수: 유저별 평균 평점(`average_rating`)\n",
    "\n",
    "##### Steam Rating Formula\n",
    "\n",
    "$$ \n",
    "\\begin{aligned} \n",
    "  \\text{average rating} =& \\frac{\\text{num of positive}}{\\text{num of review}} \\\\ \n",
    "  \\text{score} =& \\text{average rating} - (\\text{average rating} - 5.5) \\cdot 2^{-\\log_{10}^{\\text{num of reviews}}}\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "##### 베이지안 평균(Bayesian average)\n",
    "\n",
    "$$ \\text{weighted rating} = \\frac{v}{v+m} \\cdot R + \\frac{m}{v+m} \\cdot C $$\n",
    "\n",
    "- $R$: 유저의 실제 평균 평점\n",
    "\n",
    "- $v$: 해당 유저가 남긴 평점 수\n",
    "\n",
    "- $m$: 평점 신뢰도를 보정하기 위한 임계값 (예: 5나 10처럼 설정)\n",
    "\n",
    "- $C$: 전체 데이터의 평균 평점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steam Rating Formula 함수 정의\n",
    "def steam_rating_formula(ratings):\n",
    "    num_reviews = len(ratings)\n",
    "    # 평점의 평균 계산\n",
    "    avg_rating = ratings.mean() if len(ratings) > 0 else np.nan  # rating이 없는 경우 NaN 반환\n",
    "    \n",
    "    # Steam Rating Formula에 따라 점수 계산\n",
    "    score = avg_rating - (avg_rating - 5.5) * 2 ** (-np.log10(num_reviews + 1))\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# 베이지안 평균 함수 정의\n",
    "def bayesian_average(ratings, m=10):\n",
    "    global_avg = train['rating'].mean()  # 전체 평점의 평균\n",
    "    num_ratings = len(ratings)  # 유저가 남긴 평점 수\n",
    "    if num_ratings == 0:\n",
    "        return global_avg  # 리뷰가 없는 경우 전체 평균 반환\n",
    "    avg_rating = ratings.mean()  # 유저의 평균 평점\n",
    "    bayesian_score = (num_ratings / (num_ratings + m)) * avg_rating + (m / (num_ratings + m)) * global_avg\n",
    "    \n",
    "    return bayesian_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 평점 수 이상 매긴 유저를 담은 데이터프레임 생성 & user_id 인덱스 저장 (현재는 1로 설정: 전체 데이터)\n",
    "user_rating_df = train.groupby('user_id')['rating'].count()\n",
    "user_rating_idx = user_rating_df[user_rating_df >= 1].index\n",
    "\n",
    "# 특정 평점 수 이상 매긴 유저 정보만 담은 데이터프레임 생성\n",
    "heavy_user_df = train[train['user_id'].isin(user_rating_idx)]\n",
    "\n",
    "# 유저별 평점 분포 확인 (유저별 rating 값의 빈도수 계산) -> 매긴 적 없는 평점 level에는 0으로 대체\n",
    "rating_distribution = heavy_user_df.groupby('user_id')['rating'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# 유저별 평점 분포 및 평균 계산\n",
    "heavy_user_averages = heavy_user_df.groupby('user_id')['rating'].agg(\n",
    "    num_rating=lambda x: x.count(),\n",
    "    arithmetic_mean=np.mean,\n",
    "    harmonic_mean=lambda x: hmean(x) if (x > 0).all() else np.mean,  # 조화평균은 0이 아닌 값만 포함해야 한다. (역수를 취해야 하므로)\n",
    "    steam_rating=lambda x: steam_rating_formula(x),\n",
    "    bayesian_mean=lambda x: bayesian_average(x, m=10)\n",
    ")\n",
    "\n",
    "# 결과를 유저별 평점 분포와 함께 결합\n",
    "result_df = pd.concat((rating_distribution, heavy_user_averages), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_average_ratings(merged_df: pd.DataFrame, calculated_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 기존 데이터프레임에 'user_id'를 기준으로 평균 점수들 병합\n",
    "    merged_df = merged_df.merge(calculated_df[['num_rating', 'arithmetic_mean', 'harmonic_mean', 'steam_rating', 'bayesian_mean']],\n",
    "                                on='user_id', how='left')\n",
    "    \n",
    "    # 'average_rating' 계산\n",
    "    # 먼저 'num_rating' 기준으로 조건을 처리할 수 있는 변수 생성\n",
    "    condition_0 = merged_df['num_rating'] == 0\n",
    "    condition_1_10 = (merged_df['num_rating'] > 0) & (merged_df['num_rating'] < 10)\n",
    "    condition_10_20 = (merged_df['num_rating'] >= 10) & (merged_df['num_rating'] < 20)\n",
    "    condition_above_20 = merged_df['num_rating'] >= 20\n",
    "    \n",
    "    # 각 조건에 맞는 평균값 할당\n",
    "    merged_df['average_rating'] = 5.5  # 기본값을 5.5로 설정 (예시로)\n",
    "    # merged_df.loc[condition_0, 'average_rating'] = merged_df['bayesian_mean']\n",
    "    merged_df.loc[condition_1_10, 'average_rating'] = merged_df['steam_rating']\n",
    "    merged_df.loc[condition_10_20, 'average_rating'] = merged_df['bayesian_mean']\n",
    "    merged_df.loc[condition_above_20, 'average_rating'] = merged_df['harmonic_mean']\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = match_average_ratings(train, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. train과 test의 user_id를 집합(set) 형태로 저장하여 필요한 user_id 추출\n",
    "train_users = set(train['user_id'].unique())\n",
    "test_users = set(test['user_id'].unique())\n",
    "\n",
    "# test에만 존재하는 user_id (처음 등장하는 유저)\n",
    "new_users_in_test = test_users - train_users\n",
    "common_users_in_test = test_users & train_users  # 교집합으로 추출\n",
    "\n",
    "# 2. 처음 등장하는 유저에 대해 데모그래픽 정보를 기반으로 average_rating 계산\n",
    "def calculate_demographic_average(train_df, threshold=10):\n",
    "    # (age, location_country)별로 평균 평점 계산\n",
    "    demographic_avgs = train_df.groupby(['age', 'location_country']).apply(\n",
    "        lambda group: pd.Series({\n",
    "            'average_rating': group['steam_rating'].mean() if group['num_rating'].mean() < threshold else group['harmonic_mean'].mean()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    return demographic_avgs\n",
    "\n",
    "# demographic 평균 데이터프레임 생성\n",
    "demographic_avgs = calculate_demographic_average(train, threshold=10)\n",
    "\n",
    "# cold_start_users에는 demographic 평균을 기준으로 average_rating을 추가\n",
    "cold_start_users = test[test['user_id'].isin(new_users_in_test)].copy()\n",
    "cold_start_users = cold_start_users.merge(demographic_avgs, on=['age', 'location_country'], how='left')\n",
    "cold_start_users['average_rating'].fillna(5.5, inplace=True)\n",
    "\n",
    "# 3. common_users는 user_id를 기준으로 tmp_train에서 average_rating 값을 매핑\n",
    "average_rating_map = train.set_index('user_id')['average_rating'].to_dict()\n",
    "test['average_rating'] = test['user_id'].map(average_rating_map)\n",
    "\n",
    "# 4. 처음 등장하는 유저에 대해 계산한 average_rating 값으로 업데이트\n",
    "cold_start_map = cold_start_users.set_index('user_id')['average_rating'].to_dict()\n",
    "test.loc[test['user_id'].isin(new_users_in_test), 'average_rating'] = test['user_id'].map(cold_start_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피처 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['isbn', 'book_title', 'book_author', 'publisher', 'language', 'high_category', 'publication_range', 'user_id', 'age_range', 'location_country']\n",
    "num_col = ['rating', 'average_rating', 'user_review_counts', 'book_review_counts']\n",
    "\n",
    "for df in [train, test] :\n",
    "    for cat in cat_col :\n",
    "        df[cat] = df[cat].astype('str')\n",
    "    for num in num_col :\n",
    "        df[num] = df[num].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_col:\n",
    "    combined_values = pd.concat([train[col], test[col]]).unique()\n",
    "    train[col] = pd.Categorical(train[col], categories=combined_values).codes\n",
    "    test[col] = pd.Categorical(test[col], categories=combined_values).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 함수\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        'RMSE' : root_mean_squared_error(y_true, y_pred),\n",
    "        'MSE' : mean_squared_error(y_true, y_pred),\n",
    "        'MAE' : mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold\n",
    "def skf_train(X_data, y_data, params):\n",
    "    skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "    valid_rmse = []\n",
    "    valid_mse = []\n",
    "    valid_mae = []\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in tqdm.tqdm(enumerate(skf.split(X_data, y_data)), total = skf.n_splits) : \n",
    "        \n",
    "        # Train Set과 Valid Set 분할    \n",
    "        X_train, y_train = X_data.iloc[train_idx], y_data.iloc[train_idx]\n",
    "        X_valid, y_valid = X_data.iloc[valid_idx], y_data.iloc[valid_idx]\n",
    "        \n",
    "        train_data = Pool(data = X_train, label = y_train, cat_features = cat_col)\n",
    "        valid_data = Pool(data = X_valid, label = y_valid, cat_features = cat_col)\n",
    "        \n",
    "        cat_model = CatBoostRegressor(**params, iterations = 1000, \n",
    "                                    loss_function = 'RMSE', eval_metric = 'RMSE', \n",
    "                                    use_best_model = True, random_state = 42,\n",
    "                                    cat_features = [i for i in range(0, 10)])\n",
    "        cat_model.fit(train_data, eval_set = [train_data, valid_data], use_best_model = True, verbose = 100, early_stopping_rounds = 100)\n",
    "        \n",
    "        # 모델 RMSE\n",
    "        valid_metrics = calculate_metrics(y_valid, cat_model.predict(X_valid))\n",
    "        print(f\"Fold {fold + 1} Valid RMSE: {valid_metrics['RMSE']}\")\n",
    "        print(f\"Fold {fold + 1} Valid MSE:  {valid_metrics['MSE']}\")\n",
    "        print(f\"Fold {fold + 1} Valid MAE:  {valid_metrics['MAE']}\")\n",
    "        valid_rmse.append(valid_metrics['RMSE'])\n",
    "        valid_mse.append(valid_metrics['MSE'])\n",
    "        valid_mae.append(valid_metrics['MAE'])\n",
    "\n",
    "        wandb.log({\n",
    "            'Valid RMSE': valid_metrics['RMSE'],\n",
    "            'Valid MSE': valid_metrics['MSE'],\n",
    "            'Valid MAE': valid_metrics['MAE']\n",
    "        })\n",
    "        \n",
    "        # Predict\n",
    "        pred = cat_model.predict(test.drop(['rating'], axis = 1))\n",
    "        pred_df[f'pred_{fold}'] = pred\n",
    "        \n",
    "    print(f'RMSE 평균 : {np.array(valid_rmse).mean():.4f} \\n')\n",
    "\n",
    "    params = json.dumps(params)\n",
    "    wandb.log({\n",
    "        'Valid RMSE': np.array(valid_rmse).mean(),\n",
    "        'Valid MSE': np.array(valid_mse).mean(),\n",
    "        'Valid MAE': np.array(valid_mae).mean(),\n",
    "        'param': params,\n",
    "        'features': list(X_data.columns)\n",
    "    })\n",
    "    wandb.finish()\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold Optuna\n",
    "def optuna_train(X_data, y_data):\n",
    "    def train(X_data, y_data, params):\n",
    "        \n",
    "        # Train Set과 Valid Set 분할    \n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "        \n",
    "        train_data = Pool(data = X_train, label = y_train, cat_features = cat_col)\n",
    "        valid_data = Pool(data = X_valid, label = y_valid, cat_features = cat_col)\n",
    "        \n",
    "        cat_model = CatBoostRegressor(**params, iterations = 500, \n",
    "                                    loss_function = 'RMSE', eval_metric = 'RMSE', \n",
    "                                    use_best_model = True, random_state = 42,\n",
    "                                    cat_features = [i for i in range(0, 10)])\n",
    "        cat_model.fit(train_data, eval_set = [train_data, valid_data], use_best_model = True,\n",
    "                    verbose = 500, early_stopping_rounds = 100)\n",
    "        \n",
    "        valid_metrics = calculate_metrics(y_valid, cat_model.predict(X_valid))\n",
    "\n",
    "        return valid_metrics['RMSE']\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "            'boosting_type': 'Plain',\n",
    "            'bootstrap_type': 'MVS',\n",
    "            'devices': 'cuda',\n",
    "        }\n",
    "        return train(X_data, y_data, params=params)\n",
    "        \n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = train.drop(\n",
    "    columns = ['rating', 'age', 'num_rating', 'arithmetic_mean', 'harmonic_mean', 'steam_rating','bayesian_mean']\n",
    "), train['rating']\n",
    "test = test.drop(columns='age')\n",
    "\n",
    "if opt:\n",
    "    best_params = optuna_train(X_data, y_data)\n",
    "else:\n",
    "    best_params = {\n",
    "        'learning_rate': 0.1895759434037735, \n",
    "        'depth': 8, \n",
    "        'l2_leaf_reg': 4, \n",
    "        'colsample_bylevel': 0.6758183738140613,\n",
    "        'boosting_type': 'Plain',\n",
    "        'bootstrap_type': 'MVS',\n",
    "        'devices': 'cuda',\n",
    "    }\n",
    "pred_df = skf_train(X_data, y_data, params=best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['rating'] = (pred_df['pred_0'] + pred_df['pred_1'] + pred_df['pred_2'] + pred_df['pred_3'] + pred_df['pred_4'] + \n",
    "                               pred_df['pred_5'] + pred_df['pred_6'] + pred_df['pred_7'] + pred_df['pred_8'] + pred_df['pred_9']) / 10\n",
    "submit = sub[['user_id', 'isbn', 'rating']]\n",
    "submit['rating'] = submit['rating'].clip(1, 10)  # 1~10을 벗어나는 값 clipping\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    76699.000000\n",
       "mean         7.133368\n",
       "std          1.511893\n",
       "min          1.000000\n",
       "25%          6.337392\n",
       "50%          7.318706\n",
       "75%          8.186606\n",
       "max         10.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submit['rating'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
