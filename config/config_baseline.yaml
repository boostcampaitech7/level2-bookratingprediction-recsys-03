# config.yaml
# 직접 하이퍼파라미터를 추가하여 관리할 수 있습니다.

memo: |-
    남겨둘 메모가 있다면 여기에.
    여러 줄로도 작성 가능
    wandb 사용 시 wandb의 description으로 사용됩니다.


# 아래의 일곱 항목들은 argparser로 받은 인자를 우선적으로 사용합니다.
#   $ python main.py --config config.yaml --seed 2024
#   과 같이 실행할 경우 seed는 0이 아닌 2024로 설정됩니다.
predict: False  # 예측할 경우 True로 설정
checkpoint: 'saved/checkpoints/FM_best.pt'   # 예측 시 불러올 모델 경로
seed: 0         # 시드 고정
device: cuda    # 가능한 값 : cpu, cuda, mps
model: FM       # 모델 선택
wandb: False                            # wandb 사용 여부
wandb_project: 'book-rating-prediction' # wandb 프로젝트 이름
run_name: ''                            # wandb 실행 이름. 빈 문자열일 경우 자동 생성


model_args:     # model에 해당하는 파라미터만 실질적으로 사용됩니다.
    Image_DeepFM:
        datatype: image     # basic, context, image, text 중 image 가능
        embed_dim: 16       # sparse 벡터 및 이미지 벡터를 임베딩할 차원
        img_size: 64        # 이미지 전처리를 통해 조정할 이미지 크기 (64 -> 3x64x64)
        kernel_size: 3              # CNN_Base에서 사용할 각 레이어의 커널 사이즈
        channel_list: [8, 16, 32]   # CNN_Base에서 사용할 각 레이어의 채널 수
        stride: 2                   # CNN_Base에서 사용할 각 레이어의 스트라이드
        padding: 1                  # CNN_Base에서 사용할 각 레이어의 패딩
        cnn_batchnorm: True        # CNN_Base에서 배치 정규화 사용 여부
        cnn_dropout: 0.2            # CNN_Base에서 드롭아웃 비율
        mlp_dims: [16, 32]          # MLP_Base의 히든 레이어 차원
        batchnorm: True        # MLP_Base에서 배치 정규화 사용 여부
        dropout: 0.2            # MLP_Base에서 드롭아웃 비율
    
    Text_DeepFM:
        datatype: text      # basic, context, image, text 중 text 가능
        vector_create: False    # True: BERT를 통해 임베딩 벡터 생성 / False: 기존에 만든 벡터 사용
        embed_dim: 16           # sparse 벡터를 임베딩할 차원
        pretrained_model: 'bert-base-uncased'   # 텍스트 임베딩에 사용할 사전학습 모델
        word_dim: 768                           # 사전학습 모델을 통해 생성된 임베딩 벡터 차원
        mlp_dims: [16, 32]      # MLP_Base의 히든 레이어 차원
        batchnorm: True    # MLP_Base에서 배치 정규화 사용 여부
        dropout: 0.2        # MLP_Base에서 드롭아웃 비율
    
    CatBoost:
        datatype: context   # basic, context, image, text 중 context 가능
        cat_features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]    # 카테고리 변수에 해당하는 열 번호
        params:
            n_estimators: 300               # 트리 개수
            learning_rate: 0.1              # 학습률
            max_depth: 10                   # 트리 최대 깊이
            loss_function: 'RMSE'           # 손실 함수
            random_seed: 0                  # 랜덤 시드
            verbose: 100                    # 로그 출력 레벨
            task_type: "GPU"                # 사용 디바이스
            devices: '0'                    # 디바이스 번호
            
    XGBoost:
        datatype: context   # basic, context, image, text 중 context 가능
        params:
            n_estimators: 300               # 트리 개수
            max_depth: 10                   # 트리 최대 깊이
            learning_rate: 0.1              # 학습률
            objective: 'reg:squarederror'   # 손실 함수
            seed: 0                         # 랜덤 시드
            device: cuda                    # 사용 디바이스

    LightGBM:
        datatype: context   # basic, context, image, text 중 context 가능
        params:
            n_estimators: 300               # 트리 개수
            learning_rate: 0.1              # 학습률
            max_depth: 10                   # 트리 최대 깊이
            objective: 'regression_l2'      # 손실 함수
            random_state: 0                 # 랜덤 시드


dataset:
    data_path: ~/book/data/    # 데이터셋 경로
    valid_ratio: 0.2    # Train / Vaildation split
    stratified: True    # Stratified KFold 적용 여부

dataloader:
    batch_size: 1024    # 배치 사이즈
    shuffle: True       # 학습 데이터 셔플 여부
    num_workers: 0      # 멀티프로세서 수. 0: 메인프로세서만 사용

optimizer:
    type: Adam      # 사용가능한 optimizer: torch.optim.Optimizer 클래스 (https://pytorch.org/docs/stable/optim.html#algorithms)
    args:           # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
        lr: 5e-3            # 예) 모든 옵티마이저에서 사용되는 학습률
        weight_decay: 1e-4  # 예) Adam 등 / L2 정규화 가중치
        amsgrad: False      # 예) Adam 등 / amsgrad 사용 여부

loss: RMSELoss          # 직접 정의한 loss 클래스 또는 torch.nn.Module 클래스 (https://pytorch.org/docs/stable/nn.html#loss-functions)
sklearn_loss: root_mean_squared_error

lr_scheduler:
    use: False                  # True: 사용 / False: 사용하지 않음 (단, valid_ratio가 0일 경우 validation set이 없어 사용 불가)
    type: ReduceLROnPlateau     # 사용가능한 lr_scheduler: torch.optim.lr_scheduler 클래스 (https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
    args:                       # 사용하고자 하는 클래스의 파라미터를 참고하여 추가해주시면 되며, 관계가 없는 파라미터는 무시됩니다.
        mode: 'min'             # 예) ReduceLROnPlateau / 'min' 또는 'max'
        factor: 0.1             # 예) ReduceLROnPlateau / 학습률 감소 비율
        step_size: 10           # 예) StepLR / 학습률 감소 주기 (필수)
        gamma: 0.1              # 예) StepLR 등 / 학습률 감소 비율

metrics: [ MSELoss, MAELoss, RMSELoss ]  # 평가 지표. 직접 정의한 loss 클래스 또는 torch.nn.Module 클래스 (https://pytorch.org/docs/stable/nn.html#loss-functions)
sklearn_metrics: [root_mean_squared_error, mean_absolute_error, mean_squared_error]

train:
    epochs: 20                          # 학습 에폭 수
    n_estimators: 300                   # 트리 모델 학습 에폭 수
    max_depth: 7                        # 트리 모델의 최대 트리 깊이
    learning_rate: 0.1                  # 트리 모델의 학습률
    log_dir: saved/log                  # 로그 저장 경로
    ckpt_dir: saved/checkpoint    # 모델 저장 경로
    submit_dir: saved/submit            # 예측 저장 경로
    save_best_model: True               # True: val_loss가 최소인 모델 저장 / False: 모든 모델 저장
    resume: False                               # 이어서 학습할 경우 True
    resume_path: saved/checkpoint/FM_best.pt    # 이어서 학습할 모델 경로

optuna_trials: 100  # optuna 시행 횟수

optuna_args:    # optuna 파라미터 탐색 범위
    CatBoost:
        depth:  # 트리 최대 깊이
            name: 'depth'
            type: 'int'
            min: 3
            max: 15
        learning_rate:  # 학습률
            name: 'learning_rate'
            type: 'float'
            min: 0.01
            max: 0.3
        n_estimators:   # 트리 개수
            name: 'n_estimators'
            type: 'int'
            min: 100
            max: 1000
        l2_leaf_reg:    # l2 정규화 계수
            name: 'l2_leaf_reg'
            type: 'int'
            min: 1
            max: 10
    
    XGBoost:
        max_depth:  # 트리 최대 깊이
            name: 'max_depth'
            type: 'int'
            min: 3
            max: 15
        learning_rate:  # 학습률
            name: 'learning_rate'
            type: 'float'
            min: 0.01
            max: 0.3
        n_estimators:   # 트리 개수
            name: 'n_estimators'
            type: 'int'
            min: 100
            max: 1000
        gamma:  # 분할의 최소 손실 감소
            name: 'gamma'
            type: 'float'
            min: 0
            max: 5
    
    LightGBM:
        max_depth:  # 트리 최대 깊이
            name: 'max_depth'
            type: 'int'
            min: 3
            max: 15
        learning_rate:  # 학습률
            name: 'learning_rate'
            type: 'float'
            min: 0.01
            max: 0.3
        n_estimators:   # 트리 개수
            name: 'n_estimators'
            type: 'float'
            min: 0
            max: 1
        lambda_l2:  # l2 정규화 계수
            name: 'lambda_l2'
            type: 'float'
            min: 0
            max: 1